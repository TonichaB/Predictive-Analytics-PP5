{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Notebook 2: Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Identify and handle missing values across both datasets.\n",
        "* Detect and address outliers to ensure data integrity.\n",
        "* Remove duplicates and inconsistencies within the datasets.\n",
        "* Standardize formatting and data types for compatibility.\n",
        "* Save the cleaned datasets for further analysis and modeling.\n",
        "* Document the cleaning process for reproducibility and transparency.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* **Raw Datasets**:\n",
        "  * `house_prices_records.csv`: Contains house attribute data and sale prices for properties in Ames, Iowa.\n",
        "  * `inherited_houses.csv`: Contains attributes of four inherited properties but excludes sale prices.\n",
        "* **Saved Location**:\n",
        "  * Raw datasets are located in `outputs/datasets/raw/`.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* **Cleaned Datasets**:\n",
        "  * `cleaned_house_prices_records.csv`: Cleaned version of the house prices dataset.\n",
        "  * `cleaned_inherited_houses.csv`: Cleaned version of the inherited houses dataset.\n",
        "* **Documentation**:\n",
        "  * A summary of the cleaning process, including handling of missing values, outliers, and duplicates.\n",
        "  * Cleaned datasets saved in `outputs/datasets/cleaned/`.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* This notebook adheres to the CRISP-DM methodology's Data Preparation step.\n",
        "* Cleaning decisions (e.g., imputation methods, outlier handling) are based on data characteristics and domain knowledge.\n",
        "* The cleaned datasets will be ready for downstream tasks such as correlation analysis, feature engineering, and model training in subsequent notebooks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* It is assumed that you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Packages & Set Environment Variables\n",
        "\n",
        "* First you will need to import the numpy and pandas packages, and set the environment variables by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None\n",
        "from pandas_profiling import ProfileReport\n",
        "from feature_engine.imputation import ArbitraryNumberImputer, CategoricalImputer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Collected Data\n",
        "\n",
        "* Now that we have the required packages and environment variables set, you need to load the data previously downloaded (please see the Data Collection notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(f\"inputs/dataset/raw/house-price-20211124T154130Z-001/house-price/house_prices_records.csv\")\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_inherited = pd.read_csv(f\"inputs/dataset/raw/house-price-20211124T154130Z-001/house-price/inherited_houses.csv\")\n",
        "print(df_inherited.shape)\n",
        "df_inherited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Data Exploration\n",
        "\n",
        "* Next you will explore the dataset, check the variable types and distributing, missing levels, and what value these variables may add in the content of our first business requirement.\n",
        "* First of all you need to list the variables that are missing a value using the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You then need to run the pandas profiling report using just the var_missing_data variable as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if vars_missing_data:\n",
        "    pandas_report = ProfileReport(df=df[vars_missing_data], minimal=True)\n",
        "    pandas_report.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"Done. There are no variables that are missing data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assessing Missing Data Levels\n",
        "\n",
        "* **Purpose**: To gain an understanding of the extend and distribution of the missing data across the dataset.\n",
        "* **Steps**:\n",
        "  1. **Identify Variables with Missing Data**:\n",
        "       * Generate a list of columns that have missing values and their corresponding percentages.\n",
        "       * Use a profiling report or visualizations to analyze the distribution and patterns of missing data.\n",
        "  2. **Classify Missing Data**:\n",
        "       * Categorize missing data as either systematic (e.g., due to a specific condition) or random.\n",
        "  3. **Visualize Missing Data**:\n",
        "       * Use heatmaps or bar charts to understand patterns in missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def AssessMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute/len(df)*100 , 2)\n",
        "    df_missing_data = (pd.DataFrame(\n",
        "        data={\"RowsWithMissingData\": missing_data_absolute,\n",
        "        \"PercentageOfDataset\": missing_data_percentage,\n",
        "        \"DataType\": df.types}\n",
        "    )\n",
        "    .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
        "    .query(\"PercentageOfDataset > 0\")\n",
        "    )\n",
        "return df_missing_data\n",
        "\n",
        "AssessMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results\n",
        "\n",
        "* Summarize findings:\n",
        "  * Variables with high missing percentages (e.g., > 50%).\n",
        "  * Insights into patterns (e.g., variables missing together or clustering in certain rows).\n",
        "  * Highlight variables that may need special attention during cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Analysis\n",
        "\n",
        "* **Purpose**: Identify relationships between variables and the target (SalePrice) to understand their potential impact on the analysis.\n",
        "* **Steps**:\n",
        "  1. **Correlation Coefficients**:\n",
        "        * Compute Pearson and Spearman correlation matrices for numerical variables.\n",
        "        * Pearson measures linear relationships, whilst Spearman evaluations monotonic relationships.\n",
        "  \n",
        "  2. **Power Predictive Score (PPS)**:\n",
        "        * Generate a PPS Matrix to capture non-linear relationships.\n",
        "  \n",
        "  3. **Visualizations**:\n",
        "        * Display heatmaps for correlation and PPS matrices, with thresholds to highlight significant relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Insights\n",
        "\n",
        "* Variables strongly correlated with the target.\n",
        "* Features that may exhibit multicollinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Variables\n",
        "\n",
        "* **Purpose**: Remove columns with excessive missing values or those deemed irrelevant to the analysis.\n",
        "* **Steps**:\n",
        "    1. Set a threshold for dropping variables (e.g., > 80% missing).\n",
        "    2. List Variables to drop based on domain knowledge or exploratory analysis.\n",
        "    3. Document the rationale for dropping each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected Outcome:\n",
        "\n",
        "* Reduced dimensionality without siginificant information loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Impute Missing Values\n",
        "\n",
        "* **Purpose**: Fill missing values to retain as much data as possible while minimizing bias.\n",
        "* **Strategies**:\n",
        "  1. **Numerical Variables**:\n",
        "    * Use mean, median, or mode imputation for numerical variables.\n",
        "    * Use domain-specific imputation for values.\n",
        "  2. **Categorical Variables**:\n",
        "    * Impute the most frequent category or use placeholders like \"Unknown\" or \"None\" for missing values.\n",
        "  3. **Pipeline**\n",
        "    * Use pipelines to streamline imputation and apply consisten transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Standardizing Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Purpose**: Ensure uniformity in data representation for easier downstream processing.\n",
        "* **Steps**:\n",
        "  1. **Rename Columns**:\n",
        "    * Use consisten naming conventions.\n",
        "  2. **Data Type Standardization**:\n",
        "    * Convert float columns with no decimals to integers.\n",
        "    * Ensure cateforical variables are encoded as category dtype.\n",
        "  3. **Date Formatting**:\n",
        "    * Standardize date formats if applicable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splitting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Purpose**: Prepare the data for model training and evaluation by splitting it into training and testing sets.\n",
        "* **Steps**:\n",
        "  1. Define the target variable.\n",
        "  2. Split the dataset into training (80%) and testing (20%) subsets.\n",
        "  3. Use stratified sampling if necessary to maintain class balance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Purpose**: Save cleaned datasets for use in subsequent stages of the project.\n",
        "* **Steps**:\n",
        "  1. Create a directory for cleaned datasets.\n",
        "  2. Save the following:\n",
        "     1. cleaned house prices dataset\n",
        "     2. cleaned inherited houses dataset\n",
        "     3. training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion & Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* If you do not need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
