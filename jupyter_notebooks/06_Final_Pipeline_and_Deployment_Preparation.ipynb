{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **Notebook 6: Final Pipeline and Deployment Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "The objective of this notebook is to consolidate the final machine learning pipeline and prepare the trained model(s) for deployment. This includes serializing the pipeline, saving outputs, and creating essential documentation for integration into the deployment environment.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* **Trained Models and Hyperparameters**\n",
    "  * Best-performing models from the model training and evaluation notebook, including their hyperparameters.\n",
    "* **Processed Dataset**\n",
    "  * Cleaned and feature-engineered datasets ready for input into the pipeline.\n",
    "* **Evaluation Metrics and Feature Importances**\n",
    "  * Outputs from the model training notebook to inform pipeline structure and deployment requirements.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **Serialized Final Pipeline**\n",
    "  * The complete pipeline, including preprocessing and the best-performing model, saved for deployment.\n",
    "* **Deployment-Ready Artifacts**\n",
    "  * Files required for model integration, such as serialized objects and configuration files.\n",
    "* **Documentation**\n",
    "  * Summary of the pipeline, deployment steps, and integration instructions.\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* This ntoebook serves as the final step before integrating the pipeline into the deployment environment.\n",
    "* The pipeline will include preprocessing steps, feature selection, and the chosen model for prediction.\n",
    "* Key deployment considerations, such as scalability and maintainability, will be addressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Predictive-Analytics-PP5/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Predictive-Analytics-PP5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "To begin, we will import all the necessary libraries required for data processing, model loading, evaluation, and output generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Dependancies\n",
    "\n",
    "To ensure a smooth workflow, we will verify that all required dependencies are installed and compatible with the current environment. The below section checks for installed packages and their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is correctly installed (version 1.4.2)\n",
      "numpy is correctly installed (version 1.24.4)\n",
      "matplotlib is correctly installed (version 3.4.3)\n",
      "seaborn is correctly installed (version 0.11.2)\n",
      "joblib is correctly installed (version 1.4.2)\n",
      "\n",
      "Installed Dependencies\n",
      "{\n",
      "    \"pandas\": \"1.4.2\",\n",
      "    \"numpy\": \"1.24.4\",\n",
      "    \"matplotlib\": \"3.4.3\",\n",
      "    \"seaborn\": \"0.11.2\",\n",
      "    \"joblib\": \"1.4.2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# List of required libraries and their versions\n",
    "required_dependencies = {\n",
    "    \"pandas\": \"1.4.2\",\n",
    "    \"numpy\": \"1.24.4\",\n",
    "    \"matplotlib\": \"3.4.3\",\n",
    "    \"seaborn\": \"0.11.2\",\n",
    "    \"joblib\": \"1.4.2\"\n",
    "}\n",
    "\n",
    "# Check installed dependancies\n",
    "installed_dependencies = {}\n",
    "for lib, version in required_dependencies.items():\n",
    "    try:\n",
    "        lib_version = __import__(lib).__version__\n",
    "        installed_dependencies[lib] = lib_version\n",
    "        if lib_version != version:\n",
    "            print(f\"{lib} version mismatch: Expected {version}, found {lib_version}\")\n",
    "        else:\n",
    "            print(f\"{lib} is correctly installed (version {version})\")\n",
    "    except ImportError:\n",
    "        print(f\"{lib} is not installed!\")\n",
    "\n",
    "# Display summary of dependencies\n",
    "print(\"\\nInstalled Dependencies\")\n",
    "print(json.dumps(installed_dependencies, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Artifacts\n",
    "\n",
    "In this step we will load the serialized models, feature importance data, evaluation metrics and any other outputs generated in the earlier notebooks. These artifacts are essential for building the final pipeline and preparing for deployment.\n",
    "\n",
    "**Artifacts to Load:**\n",
    "1. Trained Models:\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "2. Evaluation Metrics:\n",
    "   - Performance metrics for each model.\n",
    "3. Feature Importance Data:\n",
    "   - Insights into features contributing to model predictions.\n",
    "4. Testing Data:\n",
    "   - Processed test dataset with features and target.\n",
    "\n",
    "These artifacts will ensure continuity between the modeling and deployment stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "Evaluation metrics loaded successfully.\n",
      "Feature Importance data loaded successfully.\n",
      "Test features and target loaded successfully.\n",
      "Loaded Artifacts:\n",
      "\n",
      "Evaluation Metrics:\n",
      "               Model  R2 Score       MAE       MSE\n",
      "0      Random Forest  0.813480  0.122825  0.034807\n",
      "1      Decision Tree  0.747387  0.154373  0.047141\n",
      "2                KNN  0.760565  0.143803  0.044682\n",
      "3  Gradient Boosting  0.771934  0.135791  0.042560\n",
      "4            XGBoost  0.816074  0.122992  0.034323\n",
      "\n",
      "Feature Importance (Random Forest):\n",
      "             Feature  Importance\n",
      "0   num__OverallQual    0.546651\n",
      "1     num__GrLivArea    0.129142\n",
      "2    num__GarageArea    0.051851\n",
      "3      num__1stFlrSF    0.046355\n",
      "4  num__OverallScore    0.041237\n",
      "\n",
      "Feature Importance (XGBoost):\n",
      "             Feature  Importance\n",
      "0   num__OverallQual    0.783737\n",
      "1  num__OverallScore    0.060585\n",
      "2     num__GrLivArea    0.045171\n",
      "3     num__YearBuilt    0.016420\n",
      "4      num__1stFlrSF    0.015661\n",
      "\n",
      "Test Features (First 5 Rows):\n",
      "   num__LotFrontage  num__LotArea  num__OpenPorchSF  num__MasVnrArea  \\\n",
      "0          0.144140     -0.158460         -1.096169        -0.827815   \n",
      "1          1.204764      0.612540          0.517257         1.413568   \n",
      "2         -0.556568     -0.029579         -1.096169        -0.827815   \n",
      "3         -0.911425     -1.225280          0.389147        -0.827815   \n",
      "4          0.900684      0.717202         -1.096169         0.793095   \n",
      "\n",
      "   num__BsmtFinSF1  num__GrLivArea  num__1stFlrSF  num__YearBuilt  \\\n",
      "0         0.755219       -0.922794      -0.126358        0.227176   \n",
      "1         0.902910        1.808434       0.944129       -0.783836   \n",
      "2        -1.416429       -1.038836      -0.246639        1.401254   \n",
      "3         0.585846        0.425488      -0.321073        0.748988   \n",
      "4         0.899659        0.343995       1.186707       -1.207808   \n",
      "\n",
      "   num__YearRemodAdd  num__BedroomAbvGr  ...  num__BsmtUnfSF  num__GarageArea  \\\n",
      "0          -0.873470          -2.157869  ...       -0.391317        -1.006014   \n",
      "1          -0.487465          -2.157869  ...       -0.312872         1.117159   \n",
      "2           1.683818          -1.223352  ...        0.980347        -0.551048   \n",
      "3           1.683818          -2.157869  ...        0.077111        -0.266695   \n",
      "4          -1.114724          -1.223352  ...        0.061422         2.065003   \n",
      "\n",
      "   num__GarageYrBlt  num__OverallCond  num__OverallQual     num__Age  \\\n",
      "0          0.205698          2.165000         -0.088934  1986.358491   \n",
      "1          0.274443         -0.524174          1.374088  1986.358491   \n",
      "2          0.125865          0.372217         -0.820445  1986.358491   \n",
      "3          0.176869          1.268609         -0.088934  1986.358491   \n",
      "4          0.305489         -0.524174          2.105599  1986.358491   \n",
      "\n",
      "   num__LivingLotRatio  num__FinishedBsmtRatio  num__OverallScore  \\\n",
      "0             0.798551                0.515713          35.037736   \n",
      "1             0.798551                0.515713          35.037736   \n",
      "2             0.798551                0.515713          35.037736   \n",
      "3             0.798551                0.515713          35.037736   \n",
      "4             0.798551                0.515713          35.037736   \n",
      "\n",
      "   cat__HasPorch_1  \n",
      "0         0.566038  \n",
      "1         0.566038  \n",
      "2         0.566038  \n",
      "3         0.566038  \n",
      "4         0.566038  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Test Target (First 5 Rows):\n",
      "   LogSalePrice\n",
      "0     11.947949\n",
      "1     12.691580\n",
      "2     11.652687\n",
      "3     11.976659\n",
      "4     12.661914\n"
     ]
    }
   ],
   "source": [
    "# Define paths for saved artifacts\n",
    "artifacts_paths = {\n",
    "    \"random_forest_model\": \"outputs/models/random_forest_model.pkl\",\n",
    "    \"xgboost_model\": \"outputs/models/xgboost_model.pkl\",\n",
    "    \"evaluation_metrics\": \"outputs/metrics/evaluation_metrics.csv\",\n",
    "    \"feature_importance_rf\": \"outputs/feature_importance/random_forest_feature_importance.csv\",\n",
    "    \"feature_importance_xgb\": \"outputs/feature_importance/xgboost_feature_importance.csv\",\n",
    "    \"test_features\": \"outputs/datasets/processed/final/x_test_final.csv\",\n",
    "    \"test_target\": \"outputs/datasets/processed/final/y_test_final.csv\",\n",
    "}\n",
    "\n",
    "# Load Models\n",
    "try:\n",
    "    rf_model = joblib.load(artifacts_paths[\"random_forest_model\"])\n",
    "    xgb_model = joblib.load(artifacts_paths[\"xgboost_model\"])\n",
    "    print(\"Models loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "try:\n",
    "    evaluation_metrics = pd.read_csv(artifacts_paths[\"evaluation_metrics\"])\n",
    "    print(\"Evaluation metrics loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading evaluation metrics: {e}\")\n",
    "\n",
    "# Load feature importance data\n",
    "try:\n",
    "    feature_importance_rf = pd.read_csv(artifacts_paths[\"feature_importance_rf\"])\n",
    "    feature_importance_xgb = pd.read_csv(artifacts_paths[\"feature_importance_xgb\"])\n",
    "    print(\"Feature Importance data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading feature importance data: {e}\")\n",
    "\n",
    "# Load test features and target\n",
    "try:\n",
    "    test_features = pd.read_csv(artifacts_paths[\"test_features\"])\n",
    "    test_target = pd.read_csv(artifacts_paths[\"test_target\"])\n",
    "    print(\"Test features and target loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading test data: {e}\")\n",
    "\n",
    "# Display loaded artifacts \n",
    "print(\"Loaded Artifacts:\")\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(evaluation_metrics.head())\n",
    "print(\"\\nFeature Importance (Random Forest):\")\n",
    "print(feature_importance_rf.head())\n",
    "print(\"\\nFeature Importance (XGBoost):\")\n",
    "print(feature_importance_xgb.head())\n",
    "print(\"\\nTest Features (First 5 Rows):\")\n",
    "print(test_features.head())\n",
    "print(\"\\nTest Target (First 5 Rows):\")\n",
    "print(test_target.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline\n",
    "\n",
    "**Objectives**\n",
    "The preprocessing pipeline ensures that the test data is appropriately prepared before being passed to the models for predictions. This involves verifying scaling, transformations, and alignment with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Features (First 5 Rows):\n",
      "   num__LotFrontage  num__LotArea  num__OpenPorchSF  num__MasVnrArea  \\\n",
      "0          0.144140     -0.158460         -1.096169        -0.827815   \n",
      "1          1.204764      0.612540          0.517257         1.413568   \n",
      "2         -0.556568     -0.029579         -1.096169        -0.827815   \n",
      "3         -0.911425     -1.225280          0.389147        -0.827815   \n",
      "4          0.900684      0.717202         -1.096169         0.793095   \n",
      "\n",
      "   num__BsmtFinSF1  num__GrLivArea  num__1stFlrSF  num__YearBuilt  \\\n",
      "0         0.755219       -0.922794      -0.126358        0.227176   \n",
      "1         0.902910        1.808434       0.944129       -0.783836   \n",
      "2        -1.416429       -1.038836      -0.246639        1.401254   \n",
      "3         0.585846        0.425488      -0.321073        0.748988   \n",
      "4         0.899659        0.343995       1.186707       -1.207808   \n",
      "\n",
      "   num__YearRemodAdd  num__BedroomAbvGr  ...  num__BsmtUnfSF  num__GarageArea  \\\n",
      "0          -0.873470          -2.157869  ...       -0.391317        -1.006014   \n",
      "1          -0.487465          -2.157869  ...       -0.312872         1.117159   \n",
      "2           1.683818          -1.223352  ...        0.980347        -0.551048   \n",
      "3           1.683818          -2.157869  ...        0.077111        -0.266695   \n",
      "4          -1.114724          -1.223352  ...        0.061422         2.065003   \n",
      "\n",
      "   num__GarageYrBlt  num__OverallCond  num__OverallQual     num__Age  \\\n",
      "0          0.205698          2.165000         -0.088934  1986.358491   \n",
      "1          0.274443         -0.524174          1.374088  1986.358491   \n",
      "2          0.125865          0.372217         -0.820445  1986.358491   \n",
      "3          0.176869          1.268609         -0.088934  1986.358491   \n",
      "4          0.305489         -0.524174          2.105599  1986.358491   \n",
      "\n",
      "   num__LivingLotRatio  num__FinishedBsmtRatio  num__OverallScore  \\\n",
      "0             0.798551                0.515713          35.037736   \n",
      "1             0.798551                0.515713          35.037736   \n",
      "2             0.798551                0.515713          35.037736   \n",
      "3             0.798551                0.515713          35.037736   \n",
      "4             0.798551                0.515713          35.037736   \n",
      "\n",
      "   cat__HasPorch_1  \n",
      "0         0.566038  \n",
      "1         0.566038  \n",
      "2         0.566038  \n",
      "3         0.566038  \n",
      "4         0.566038  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Test Target (First 5 Rows):\n",
      "   LogSalePrice\n",
      "0     11.947949\n",
      "1     12.691580\n",
      "2     11.652687\n",
      "3     11.976659\n",
      "4     12.661914\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of test features\n",
    "print(\"Test Features (First 5 Rows):\")\n",
    "print(test_features.head())\n",
    "\n",
    "print(\"\\nTest Target (First 5 Rows):\")\n",
    "print(test_target.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**\n",
    "\n",
    "The following steps were conducted to validate the preprocessing:\n",
    "\n",
    "1. **Feature Scaling:** The test features were checked, and it was confirmed that scaling had already been applied during earlier preprocessing steps. The values exhibit a standardized format (mean near 0 and consistent range).\n",
    "2. **Target Transformation:** The target variable (`LogSalePrice`) was confirmed to retain its log-transformed format as expected.\n",
    "3. **Data Readiness:** Both features and target data are aligned with the requirements of the trained models.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "No additional preprocessing steps are required. The test data is ready to be used directly in the pipeline for model integration and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Integration\n",
    "\n",
    "This section demonstrates the integration of trained models to generate predictions for the testing dataset. The primary objective is to validate the models by making predictions on unseen data and prepare the results for further evaluation.\n",
    "\n",
    "**Process:**\n",
    "1. **Inegration Function:** A custom function `integrate_model` will be implemented to streamline the process of applying models to the testing dataset. This function:\n",
    "   - Accepts a trained model, features, and a model name.\n",
    "   - Generates predictions using the provided model.\n",
    "   - Returns a DataFrame containing the predictions alongside the model name for traceability.\n",
    "2. **Predictions for Each Model:**\n",
    "   - Predictions will be generated using the two selected models:\n",
    "     - Random Forest\n",
    "     - XGBoost\n",
    "3. **Combined Predictions:**\n",
    "   - The predictions from both models will be consolidated into a single DataFrame for comparative analysis.\n",
    "4. **Validation Against Actual Target Values:**\n",
    "   - The Mean Absolute Error (MAE) metric will be calculated for each model by comparing their predictions against the actual test target values (`LogSalePrice`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Predictions:\n",
      "           Model  Predicted LogSalePrice\n",
      "0  Random Forest               11.862418\n",
      "1  Random Forest               12.722941\n",
      "2  Random Forest               11.669759\n",
      "3  Random Forest               11.940186\n",
      "4  Random Forest               12.666655\n",
      "XGBoost Model Predictions:\n",
      "     Model  Predicted LogSalePrice\n",
      "0  XGBoost               11.869076\n",
      "1  XGBoost               12.791644\n",
      "2  XGBoost               11.692052\n",
      "3  XGBoost               11.985173\n",
      "4  XGBoost               12.771486\n",
      "\n",
      "Combined Model Predictions:\n",
      "           Model  Predicted LogSalePrice\n",
      "0  Random Forest               11.862418\n",
      "1  Random Forest               12.722941\n",
      "2  Random Forest               11.669759\n",
      "3  Random Forest               11.940186\n",
      "4  Random Forest               12.666655\n"
     ]
    }
   ],
   "source": [
    "# Define a function for model integration\n",
    "def integrate_model(model, features, model_name):\n",
    "    \"\"\"\n",
    "    Integrates a model to make predictions on given features.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model to use for predictions.\n",
    "        features: DataFrame of features to predict on.\n",
    "        model_name: Name of the model for logging and clarity.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions and corresponding model name.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(features)\n",
    "    results = pd.DataFrame({\n",
    "        \"Model\": [model_name] * len(predictions),\n",
    "        \"Predicted LogSalePrice\": predictions\n",
    "    })\n",
    "    return results\n",
    "\n",
    "# Integrate Random Forest Model\n",
    "rf_predictions = integrate_model(rf_model, test_features, \"Random Forest\")\n",
    "print(\"Random Forest Model Predictions:\")\n",
    "print(rf_predictions.head())\n",
    "\n",
    "# Integrate XGBoost Model\n",
    "xgb_predictions = integrate_model(xgb_model, test_features, \"XGBoost\")\n",
    "print(\"XGBoost Model Predictions:\")\n",
    "print(xgb_predictions.head())\n",
    "\n",
    "# Combine predictions into a single DataFrame for comparison\n",
    "combined_predictions = pd.concat([rf_predictions, xgb_predictions], axis=0)\n",
    "print(\"\\nCombined Model Predictions:\")\n",
    "print(combined_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAE: 0.1228245674095155\n",
      "XGBoost MAE: 0.1288523556757043\n"
     ]
    }
   ],
   "source": [
    "# Validate Predictions Against Actuals\n",
    "rf_mae = mean_absolute_error(test_target, rf_predictions[\"Predicted LogSalePrice\"])\n",
    "xgb_mae = mean_absolute_error(test_target, xgb_predictions[\"Predicted LogSalePrice\"])\n",
    "\n",
    "print(f\"Random Forest MAE: {rf_mae}\")\n",
    "print(f\"XGBoost MAE: {xgb_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- The **Random Forest** model achieved a lower MAE compared to **XGBoost**, indicating slightly better predictive performance.\n",
    "- Both models provided consistent predictions, aligning with their previously evaluated performance during model testing and tuning.\n",
    "- The predictions and validation results confirmed the effectiveness of both models on unseen data, with the **Random Forest** model slightly outperforming **XGBoost** in terms of MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Other Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
