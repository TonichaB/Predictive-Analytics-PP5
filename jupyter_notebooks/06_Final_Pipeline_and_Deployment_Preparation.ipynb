{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **Notebook 6: Final Pipeline and Deployment Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "The objective of this notebook is to consolidate the final machine learning pipeline and prepare the trained model(s) for deployment. This includes serializing the pipeline, saving outputs, and creating essential documentation for integration into the deployment environment.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* **Trained Models and Hyperparameters**\n",
    "  * Best-performing models from the model training and evaluation notebook, including their hyperparameters.\n",
    "* **Processed Dataset**\n",
    "  * Cleaned and feature-engineered datasets ready for input into the pipeline.\n",
    "* **Evaluation Metrics and Feature Importances**\n",
    "  * Outputs from the model training notebook to inform pipeline structure and deployment requirements.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **Serialized Final Pipeline**\n",
    "  * The complete pipeline, including preprocessing and the best-performing model, saved for deployment.\n",
    "* **Deployment-Ready Artifacts**\n",
    "  * Files required for model integration, such as serialized objects and configuration files.\n",
    "* **Documentation**\n",
    "  * Summary of the pipeline, deployment steps, and integration instructions.\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* This ntoebook serves as the final step before integrating the pipeline into the deployment environment.\n",
    "* The pipeline will include preprocessing steps, feature selection, and the chosen model for prediction.\n",
    "* Key deployment considerations, such as scalability and maintainability, will be addressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Predictive-Analytics-PP5/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Predictive-Analytics-PP5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "To begin, we will import all the necessary libraries required for data processing, model loading, evaluation, and output generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Dependancies\n",
    "\n",
    "To ensure a smooth workflow, we will verify that all required dependencies are installed and compatible with the current environment. The below section checks for installed packages and their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is correctly installed (version 1.4.2)\n",
      "numpy is correctly installed (version 1.24.4)\n",
      "matplotlib is correctly installed (version 3.4.3)\n",
      "seaborn is correctly installed (version 0.11.2)\n",
      "joblib is correctly installed (version 1.4.2)\n",
      "\n",
      "Installed Dependencies\n",
      "{\n",
      "    \"pandas\": \"1.4.2\",\n",
      "    \"numpy\": \"1.24.4\",\n",
      "    \"matplotlib\": \"3.4.3\",\n",
      "    \"seaborn\": \"0.11.2\",\n",
      "    \"joblib\": \"1.4.2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# List of required libraries and their versions\n",
    "required_dependencies = {\n",
    "    \"pandas\": \"1.4.2\",\n",
    "    \"numpy\": \"1.24.4\",\n",
    "    \"matplotlib\": \"3.4.3\",\n",
    "    \"seaborn\": \"0.11.2\",\n",
    "    \"joblib\": \"1.4.2\"\n",
    "}\n",
    "\n",
    "# Check installed dependancies\n",
    "installed_dependencies = {}\n",
    "for lib, version in required_dependencies.items():\n",
    "    try:\n",
    "        lib_version = __import__(lib).__version__\n",
    "        installed_dependencies[lib] = lib_version\n",
    "        if lib_version != version:\n",
    "            print(f\"{lib} version mismatch: Expected {version}, found {lib_version}\")\n",
    "        else:\n",
    "            print(f\"{lib} is correctly installed (version {version})\")\n",
    "    except ImportError:\n",
    "        print(f\"{lib} is not installed!\")\n",
    "\n",
    "# Display summary of dependencies\n",
    "print(\"\\nInstalled Dependencies\")\n",
    "print(json.dumps(installed_dependencies, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Artifacts\n",
    "\n",
    "In this step we will load the serialized models, feature importance data, evaluation metrics and any other outputs generated in the earlier notebooks. These artifacts are essential for building the final pipeline and preparing for deployment.\n",
    "\n",
    "**Artifacts to Load:**\n",
    "1. Trained Models:\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "2. Evaluation Metrics:\n",
    "   - Performance metrics for each model.\n",
    "3. Feature Importance Data:\n",
    "   - Insights into features contributing to model predictions.\n",
    "4. Testing Data:\n",
    "   - Processed test dataset with features and target.\n",
    "\n",
    "These artifacts will ensure continuity between the modeling and deployment stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "Evaluation metrics loaded successfully.\n",
      "Feature Importance data loaded successfully.\n",
      "Test data loaded successfully.\n",
      "Loaded Artifacts:\n",
      "\n",
      "Evaluation Metrics:\n",
      "               Model  R2 Score       MAE       MSE\n",
      "0      Random Forest  0.813480  0.122825  0.034807\n",
      "1      Decision Tree  0.747387  0.154373  0.047141\n",
      "2                KNN  0.760565  0.143803  0.044682\n",
      "3  Gradient Boosting  0.771934  0.135791  0.042560\n",
      "4            XGBoost  0.816074  0.122992  0.034323\n",
      "\n",
      "Feature Importance (Random Forest):\n",
      "             Feature  Importance\n",
      "0   num__OverallQual    0.546651\n",
      "1     num__GrLivArea    0.129142\n",
      "2    num__GarageArea    0.051851\n",
      "3      num__1stFlrSF    0.046355\n",
      "4  num__OverallScore    0.041237\n",
      "\n",
      "Feature Importance (XGBoost):\n",
      "             Feature  Importance\n",
      "0   num__OverallQual    0.783737\n",
      "1  num__OverallScore    0.060585\n",
      "2     num__GrLivArea    0.045171\n",
      "3     num__YearBuilt    0.016420\n",
      "4      num__1stFlrSF    0.015661\n",
      "\n",
      "Test Data:\n",
      "   num__LotFrontage  num__LotArea  num__OpenPorchSF  num__MasVnrArea  \\\n",
      "0          0.144140     -0.158460         -1.096169        -0.827815   \n",
      "1          1.204764      0.612540          0.517257         1.413568   \n",
      "2         -0.556568     -0.029579         -1.096169        -0.827815   \n",
      "3         -0.911425     -1.225280          0.389147        -0.827815   \n",
      "4          0.900684      0.717202         -1.096169         0.793095   \n",
      "\n",
      "   num__BsmtFinSF1  num__GrLivArea  num__1stFlrSF  num__YearBuilt  \\\n",
      "0         0.755219       -0.922794      -0.126358        0.227176   \n",
      "1         0.902910        1.808434       0.944129       -0.783836   \n",
      "2        -1.416429       -1.038836      -0.246639        1.401254   \n",
      "3         0.585846        0.425488      -0.321073        0.748988   \n",
      "4         0.899659        0.343995       1.186707       -1.207808   \n",
      "\n",
      "   num__YearRemodAdd  num__BedroomAbvGr  ...  num__GarageArea  \\\n",
      "0          -0.873470          -2.157869  ...        -1.006014   \n",
      "1          -0.487465          -2.157869  ...         1.117159   \n",
      "2           1.683818          -1.223352  ...        -0.551048   \n",
      "3           1.683818          -2.157869  ...        -0.266695   \n",
      "4          -1.114724          -1.223352  ...         2.065003   \n",
      "\n",
      "   num__GarageYrBlt  num__OverallCond  num__OverallQual          Age  \\\n",
      "0          0.205698          2.165000         -0.088934  1986.358491   \n",
      "1          0.274443         -0.524174          1.374088  1986.358491   \n",
      "2          0.125865          0.372217         -0.820445  1986.358491   \n",
      "3          0.176869          1.268609         -0.088934  1986.358491   \n",
      "4          0.305489         -0.524174          2.105599  1986.358491   \n",
      "\n",
      "   LivingLotRatio  FinishedBsmtRatio  OverallScore  HasPorch  LogSalePrice  \n",
      "0        0.798551           0.515713     35.037736  0.566038     11.947949  \n",
      "1        0.798551           0.515713     35.037736  0.566038     12.691580  \n",
      "2        0.798551           0.515713     35.037736  0.566038     11.652687  \n",
      "3        0.798551           0.515713     35.037736  0.566038     11.976659  \n",
      "4        0.798551           0.515713     35.037736  0.566038     12.661914  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define paths for saved artifacts\n",
    "artifacts_paths = {\n",
    "    \"random_forest_model\": \"outputs/models/random_forest_model.pkl\",\n",
    "    \"xgboost_model\": \"outputs/models/xgboost_model.pkl\",\n",
    "    \"evaluation_metrics\": \"outputs/metrics/evaluation_metrics.csv\",\n",
    "    \"feature_importance_rf\": \"outputs/feature_importance/random_forest_feature_importance.csv\",\n",
    "    \"feature_importance_xgb\": \"outputs/feature_importance/xgboost_feature_importance.csv\",\n",
    "    \"test_data\": \"outputs/datasets/processed/with_target/test_with_target.csv\",\n",
    "}\n",
    "\n",
    "# Load Models\n",
    "try:\n",
    "    rf_model = joblib.load(artifacts_paths[\"random_forest_model\"])\n",
    "    xgb_model = joblib.load(artifacts_paths[\"xgboost_model\"])\n",
    "    print(\"Models loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "try:\n",
    "    evaluation_metrics = pd.read_csv(artifacts_paths[\"evaluation_metrics\"])\n",
    "    print(\"Evaluation metrics loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading evaluation metrics: {e}\")\n",
    "\n",
    "# Load feature importance data\n",
    "try:\n",
    "    feature_importance_rf = pd.read_csv(artifacts_paths[\"feature_importance_rf\"])\n",
    "    feature_importance_xgb = pd.read_csv(artifacts_paths[\"feature_importance_xgb\"])\n",
    "    print(\"Feature Importance data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading feature importance data: {e}\")\n",
    "\n",
    "# Load test data\n",
    "try:\n",
    "    test_data = pd.read_csv(artifacts_paths[\"test_data\"])\n",
    "    print(\"Test data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading test data: {e}\")\n",
    "\n",
    "# Display loaded artifacts \n",
    "print(\"Loaded Artifacts:\")\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(evaluation_metrics.head())\n",
    "print(\"\\nFeature Importance (Random Forest):\")\n",
    "print(feature_importance_rf.head())\n",
    "print(\"\\nFeature Importance (XGBoost):\")\n",
    "print(feature_importance_xgb.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Intergration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Other Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
