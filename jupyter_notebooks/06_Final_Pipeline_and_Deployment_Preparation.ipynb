{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **Notebook 6: Final Pipeline and Deployment Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "The objective of this notebook is to consolidate the final machine learning pipeline and prepare the trained model(s) for deployment. This includes serializing the pipeline, saving outputs, and creating essential documentation for integration into the deployment environment.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* **Trained Models and Hyperparameters**\n",
    "  * Best-performing models from the model training and evaluation notebook, including their hyperparameters.\n",
    "* **Processed Dataset**\n",
    "  * Cleaned and feature-engineered datasets ready for input into the pipeline.\n",
    "* **Evaluation Metrics and Feature Importances**\n",
    "  * Outputs from the model training notebook to inform pipeline structure and deployment requirements.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **Serialized Final Pipeline**\n",
    "  * The complete pipeline, including preprocessing and the best-performing model, saved for deployment.\n",
    "* **Deployment-Ready Artifacts**\n",
    "  * Files required for model integration, such as serialized objects and configuration files.\n",
    "* **Documentation**\n",
    "  * Summary of the pipeline, deployment steps, and integration instructions.\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* This ntoebook serves as the final step before integrating the pipeline into the deployment environment.\n",
    "* The pipeline will include preprocessing steps, feature selection, and the chosen model for prediction.\n",
    "* Key deployment considerations, such as scalability and maintainability, will be addressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Predictive-Analytics-PP5/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Predictive-Analytics-PP5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "To begin, we will import all the necessary libraries required for data processing, model loading, evaluation, and output generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Dependancies\n",
    "\n",
    "To ensure a smooth workflow, we will verify that all required dependencies are installed and compatible with the current environment. The below section checks for installed packages and their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is correctly installed (version 1.4.2)\n",
      "numpy is correctly installed (version 1.24.4)\n",
      "matplotlib is correctly installed (version 3.4.3)\n",
      "seaborn is correctly installed (version 0.11.2)\n",
      "joblib is correctly installed (version 1.4.2)\n",
      "\n",
      "Installed Dependencies\n",
      "{\n",
      "    \"pandas\": \"1.4.2\",\n",
      "    \"numpy\": \"1.24.4\",\n",
      "    \"matplotlib\": \"3.4.3\",\n",
      "    \"seaborn\": \"0.11.2\",\n",
      "    \"joblib\": \"1.4.2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# List of required libraries and their versions\n",
    "required_dependencies = {\n",
    "    \"pandas\": \"1.4.2\",\n",
    "    \"numpy\": \"1.24.4\",\n",
    "    \"matplotlib\": \"3.4.3\",\n",
    "    \"seaborn\": \"0.11.2\",\n",
    "    \"joblib\": \"1.4.2\"\n",
    "}\n",
    "\n",
    "# Check installed dependancies\n",
    "installed_dependencies = {}\n",
    "for lib, version in required_dependencies.items():\n",
    "    try:\n",
    "        lib_version = __import__(lib).__version__\n",
    "        installed_dependencies[lib] = lib_version\n",
    "        if lib_version != version:\n",
    "            print(f\"{lib} version mismatch: Expected {version}, found {lib_version}\")\n",
    "        else:\n",
    "            print(f\"{lib} is correctly installed (version {version})\")\n",
    "    except ImportError:\n",
    "        print(f\"{lib} is not installed!\")\n",
    "\n",
    "# Display summary of dependencies\n",
    "print(\"\\nInstalled Dependencies\")\n",
    "print(json.dumps(installed_dependencies, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Artifacts\n",
    "\n",
    "In this step we will load the serialized models, feature importance data, evaluation metrics and any other outputs generated in the earlier notebooks. These artifacts are essential for building the final pipeline and preparing for deployment.\n",
    "\n",
    "**Artifacts to Load:**\n",
    "1. Trained Models:\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "2. Evaluation Metrics:\n",
    "   - Performance metrics for each model.\n",
    "3. Feature Importance Data:\n",
    "   - Insights into features contributing to model predictions.\n",
    "4. Testing Data:\n",
    "   - Processed test dataset with features and target.\n",
    "\n",
    "These artifacts will ensure continuity between the modeling and deployment stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "Evaluation metrics loaded successfully.\n",
      "Feature Importance data loaded successfully.\n",
      "Test features and target loaded successfully.\n",
      "Loaded Artifacts:\n",
      "\n",
      "Evaluation Metrics:\n",
      "               Model  R2 Score       MAE       MSE\n",
      "0      Random Forest  0.813480  0.122825  0.034807\n",
      "1      Decision Tree  0.747387  0.154373  0.047141\n",
      "2                KNN  0.760565  0.143803  0.044682\n",
      "3  Gradient Boosting  0.771934  0.135791  0.042560\n",
      "4            XGBoost  0.816074  0.122992  0.034323\n",
      "\n",
      "Feature Importance (Random Forest):\n",
      "             Feature  Importance\n",
      "0   num__OverallQual    0.546662\n",
      "1     num__GrLivArea    0.129170\n",
      "2    num__GarageArea    0.052238\n",
      "3      num__1stFlrSF    0.046122\n",
      "4  num__OverallScore    0.041399\n",
      "\n",
      "Feature Importance (XGBoost):\n",
      "             Feature  Importance\n",
      "0   num__OverallQual    0.783737\n",
      "1  num__OverallScore    0.060585\n",
      "2     num__GrLivArea    0.045171\n",
      "3     num__YearBuilt    0.016420\n",
      "4      num__1stFlrSF    0.015661\n",
      "\n",
      "Test Features (First 5 Rows):\n",
      "   num__LotFrontage  num__LotArea  num__OpenPorchSF  num__MasVnrArea  \\\n",
      "0          0.144140     -0.158460         -1.096169        -0.827815   \n",
      "1          1.204764      0.612540          0.517257         1.413568   \n",
      "2         -0.556568     -0.029579         -1.096169        -0.827815   \n",
      "3         -0.911425     -1.225280          0.389147        -0.827815   \n",
      "4          0.900684      0.717202         -1.096169         0.793095   \n",
      "\n",
      "   num__BsmtFinSF1  num__GrLivArea  num__1stFlrSF  num__YearBuilt  \\\n",
      "0         0.755219       -0.922794      -0.126358        0.227176   \n",
      "1         0.902910        1.808434       0.944129       -0.783836   \n",
      "2        -1.416429       -1.038836      -0.246639        1.401254   \n",
      "3         0.585846        0.425488      -0.321073        0.748988   \n",
      "4         0.899659        0.343995       1.186707       -1.207808   \n",
      "\n",
      "   num__YearRemodAdd  num__BedroomAbvGr  ...  num__BsmtUnfSF  num__GarageArea  \\\n",
      "0          -0.873470          -2.157869  ...       -0.391317        -1.006014   \n",
      "1          -0.487465          -2.157869  ...       -0.312872         1.117159   \n",
      "2           1.683818          -1.223352  ...        0.980347        -0.551048   \n",
      "3           1.683818          -2.157869  ...        0.077111        -0.266695   \n",
      "4          -1.114724          -1.223352  ...        0.061422         2.065003   \n",
      "\n",
      "   num__GarageYrBlt  num__OverallCond  num__OverallQual  num__Age  \\\n",
      "0          0.205698          2.165000         -0.088934 -0.227176   \n",
      "1          0.274443         -0.524174          1.374088  0.783836   \n",
      "2          0.125865          0.372217         -0.820445 -1.401254   \n",
      "3          0.176869          1.268609         -0.088934 -0.748988   \n",
      "4          0.305489         -0.524174          2.105599  1.207808   \n",
      "\n",
      "   num__LivingLotRatio  num__FinishedBsmtRatio  num__OverallScore  \\\n",
      "0            -0.621623                0.841719           1.506202   \n",
      "1             0.792606                0.720328           0.642186   \n",
      "2            -0.830326               -1.415946          -0.437833   \n",
      "3             1.552164                0.541291           0.858190   \n",
      "4            -0.420838                0.856249           1.182196   \n",
      "\n",
      "   cat__HasPorch  \n",
      "0            0.0  \n",
      "1            1.0  \n",
      "2            0.0  \n",
      "3            1.0  \n",
      "4            0.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Test Target (First 5 Rows):\n",
      "   LogSalePrice\n",
      "0     11.947949\n",
      "1     12.691580\n",
      "2     11.652687\n",
      "3     11.976659\n",
      "4     12.661914\n"
     ]
    }
   ],
   "source": [
    "# Define paths for saved artifacts\n",
    "artifacts_paths = {\n",
    "    \"random_forest_model\": \"outputs/models/random_forest_model.pkl\",\n",
    "    \"xgboost_model\": \"outputs/models/xgboost_model.pkl\",\n",
    "    \"evaluation_metrics\": \"outputs/metrics/evaluation_metrics.csv\",\n",
    "    \"feature_importance_rf\": \"outputs/feature_importance/random_forest_feature_importance.csv\",\n",
    "    \"feature_importance_xgb\": \"outputs/feature_importance/xgboost_feature_importance.csv\",\n",
    "    \"test_features\": \"outputs/datasets/processed/final/x_test_final.csv\",\n",
    "    \"test_target\": \"outputs/datasets/processed/final/y_test_final.csv\",\n",
    "}\n",
    "\n",
    "# Load Models\n",
    "try:\n",
    "    rf_model = joblib.load(artifacts_paths[\"random_forest_model\"])\n",
    "    xgb_model = joblib.load(artifacts_paths[\"xgboost_model\"])\n",
    "    print(\"Models loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "try:\n",
    "    evaluation_metrics = pd.read_csv(artifacts_paths[\"evaluation_metrics\"])\n",
    "    print(\"Evaluation metrics loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading evaluation metrics: {e}\")\n",
    "\n",
    "# Load feature importance data\n",
    "try:\n",
    "    feature_importance_rf = pd.read_csv(artifacts_paths[\"feature_importance_rf\"])\n",
    "    feature_importance_xgb = pd.read_csv(artifacts_paths[\"feature_importance_xgb\"])\n",
    "    print(\"Feature Importance data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading feature importance data: {e}\")\n",
    "\n",
    "# Load test features and target\n",
    "try:\n",
    "    test_features = pd.read_csv(artifacts_paths[\"test_features\"])\n",
    "    test_target = pd.read_csv(artifacts_paths[\"test_target\"])\n",
    "    print(\"Test features and target loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading test data: {e}\")\n",
    "\n",
    "# Display loaded artifacts \n",
    "print(\"Loaded Artifacts:\")\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(evaluation_metrics.head())\n",
    "print(\"\\nFeature Importance (Random Forest):\")\n",
    "print(feature_importance_rf.head())\n",
    "print(\"\\nFeature Importance (XGBoost):\")\n",
    "print(feature_importance_xgb.head())\n",
    "print(\"\\nTest Features (First 5 Rows):\")\n",
    "print(test_features.head())\n",
    "print(\"\\nTest Target (First 5 Rows):\")\n",
    "print(test_target.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline\n",
    "\n",
    "**Objectives**\n",
    "The preprocessing pipeline ensures that the test data is appropriately prepared before being passed to the models for predictions. This involves verifying scaling, transformations, and alignment with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Features (First 5 Rows):\n",
      "   num__LotFrontage  num__LotArea  num__OpenPorchSF  num__MasVnrArea  \\\n",
      "0          0.144140     -0.158460         -1.096169        -0.827815   \n",
      "1          1.204764      0.612540          0.517257         1.413568   \n",
      "2         -0.556568     -0.029579         -1.096169        -0.827815   \n",
      "3         -0.911425     -1.225280          0.389147        -0.827815   \n",
      "4          0.900684      0.717202         -1.096169         0.793095   \n",
      "\n",
      "   num__BsmtFinSF1  num__GrLivArea  num__1stFlrSF  num__YearBuilt  \\\n",
      "0         0.755219       -0.922794      -0.126358        0.227176   \n",
      "1         0.902910        1.808434       0.944129       -0.783836   \n",
      "2        -1.416429       -1.038836      -0.246639        1.401254   \n",
      "3         0.585846        0.425488      -0.321073        0.748988   \n",
      "4         0.899659        0.343995       1.186707       -1.207808   \n",
      "\n",
      "   num__YearRemodAdd  num__BedroomAbvGr  ...  num__BsmtUnfSF  num__GarageArea  \\\n",
      "0          -0.873470          -2.157869  ...       -0.391317        -1.006014   \n",
      "1          -0.487465          -2.157869  ...       -0.312872         1.117159   \n",
      "2           1.683818          -1.223352  ...        0.980347        -0.551048   \n",
      "3           1.683818          -2.157869  ...        0.077111        -0.266695   \n",
      "4          -1.114724          -1.223352  ...        0.061422         2.065003   \n",
      "\n",
      "   num__GarageYrBlt  num__OverallCond  num__OverallQual  num__Age  \\\n",
      "0          0.205698          2.165000         -0.088934 -0.227176   \n",
      "1          0.274443         -0.524174          1.374088  0.783836   \n",
      "2          0.125865          0.372217         -0.820445 -1.401254   \n",
      "3          0.176869          1.268609         -0.088934 -0.748988   \n",
      "4          0.305489         -0.524174          2.105599  1.207808   \n",
      "\n",
      "   num__LivingLotRatio  num__FinishedBsmtRatio  num__OverallScore  \\\n",
      "0            -0.621623                0.841719           1.506202   \n",
      "1             0.792606                0.720328           0.642186   \n",
      "2            -0.830326               -1.415946          -0.437833   \n",
      "3             1.552164                0.541291           0.858190   \n",
      "4            -0.420838                0.856249           1.182196   \n",
      "\n",
      "   cat__HasPorch  \n",
      "0            0.0  \n",
      "1            1.0  \n",
      "2            0.0  \n",
      "3            1.0  \n",
      "4            0.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Test Target (First 5 Rows):\n",
      "   LogSalePrice\n",
      "0     11.947949\n",
      "1     12.691580\n",
      "2     11.652687\n",
      "3     11.976659\n",
      "4     12.661914\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of test features\n",
    "print(\"Test Features (First 5 Rows):\")\n",
    "print(test_features.head())\n",
    "\n",
    "print(\"\\nTest Target (First 5 Rows):\")\n",
    "print(test_target.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**\n",
    "\n",
    "The following steps were conducted to validate the preprocessing:\n",
    "\n",
    "1. **Feature Scaling:** The test features were checked, and it was confirmed that scaling had already been applied during earlier preprocessing steps. The values exhibit a standardized format (mean near 0 and consistent range).\n",
    "2. **Target Transformation:** The target variable (`LogSalePrice`) was confirmed to retain its log-transformed format as expected.\n",
    "3. **Data Readiness:** Both features and target data are aligned with the requirements of the trained models.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "No additional preprocessing steps are required. The test data is ready to be used directly in the pipeline for model integration and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Integration\n",
    "\n",
    "This section demonstrates the integration of trained models to generate predictions for the testing dataset. The primary objective is to validate the models by making predictions on unseen data and prepare the results for further evaluation.\n",
    "\n",
    "**Process:**\n",
    "1. **Inegration Function:** A custom function `integrate_model` will be implemented to streamline the process of applying models to the testing dataset. This function:\n",
    "   - Accepts a trained model, features, and a model name.\n",
    "   - Generates predictions using the provided model.\n",
    "   - Returns a DataFrame containing the predictions alongside the model name for traceability.\n",
    "2. **Predictions for Each Model:**\n",
    "   - Predictions will be generated using the two selected models:\n",
    "     - Random Forest\n",
    "     - XGBoost\n",
    "3. **Combined Predictions:**\n",
    "   - The predictions from both models will be consolidated into a single DataFrame for comparative analysis.\n",
    "4. **Validation Against Actual Target Values:**\n",
    "   - The Mean Absolute Error (MAE) metric will be calculated for each model by comparing their predictions against the actual test target values (`LogSalePrice`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Predictions:\n",
      "           Model  Predicted LogSalePrice\n",
      "0  Random Forest               11.865206\n",
      "1  Random Forest               12.713614\n",
      "2  Random Forest               11.585296\n",
      "3  Random Forest               11.945785\n",
      "4  Random Forest               12.637299\n",
      "XGBoost Model Predictions:\n",
      "     Model  Predicted LogSalePrice\n",
      "0  XGBoost               11.885277\n",
      "1  XGBoost               12.795120\n",
      "2  XGBoost               11.634674\n",
      "3  XGBoost               11.983330\n",
      "4  XGBoost               12.727750\n",
      "\n",
      "Combined Model Predictions:\n",
      "           Model  Predicted LogSalePrice\n",
      "0  Random Forest               11.865206\n",
      "1  Random Forest               12.713614\n",
      "2  Random Forest               11.585296\n",
      "3  Random Forest               11.945785\n",
      "4  Random Forest               12.637299\n"
     ]
    }
   ],
   "source": [
    "# Define a function for model integration\n",
    "def integrate_model(model, features, model_name):\n",
    "    \"\"\"\n",
    "    Integrates a model to make predictions on given features.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model to use for predictions.\n",
    "        features: DataFrame of features to predict on.\n",
    "        model_name: Name of the model for logging and clarity.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions and corresponding model name.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(features)\n",
    "    results = pd.DataFrame({\n",
    "        \"Model\": [model_name] * len(predictions),\n",
    "        \"Predicted LogSalePrice\": predictions\n",
    "    })\n",
    "    return results\n",
    "\n",
    "# Integrate Random Forest Model\n",
    "rf_predictions = integrate_model(rf_model, test_features, \"Random Forest\")\n",
    "print(\"Random Forest Model Predictions:\")\n",
    "print(rf_predictions.head())\n",
    "\n",
    "# Integrate XGBoost Model\n",
    "xgb_predictions = integrate_model(xgb_model, test_features, \"XGBoost\")\n",
    "print(\"XGBoost Model Predictions:\")\n",
    "print(xgb_predictions.head())\n",
    "\n",
    "# Combine predictions into a single DataFrame for comparison\n",
    "combined_predictions = pd.concat([rf_predictions, xgb_predictions], axis=0)\n",
    "print(\"\\nCombined Model Predictions:\")\n",
    "print(combined_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAE: 0.10429243792856001\n",
      "XGBoost MAE: 0.10712006451855471\n"
     ]
    }
   ],
   "source": [
    "# Validate Predictions Against Actuals\n",
    "rf_mae = mean_absolute_error(test_target, rf_predictions[\"Predicted LogSalePrice\"])\n",
    "xgb_mae = mean_absolute_error(test_target, xgb_predictions[\"Predicted LogSalePrice\"])\n",
    "\n",
    "print(f\"Random Forest MAE: {rf_mae}\")\n",
    "print(f\"XGBoost MAE: {xgb_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- The **Random Forest** model achieved a lower MAE compared to **XGBoost**, indicating slightly better predictive performance.\n",
    "- Both models provided consistent predictions, aligning with their previously evaluated performance during model testing and tuning.\n",
    "- The predictions and validation results confirmed the effectiveness of both models on unseen data, with the **Random Forest** model slightly outperforming **XGBoost** in terms of MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Pipeline\n",
    "\n",
    "This section will focus on loading, preprocessing, and generating predictions for the inherited properties dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Load and Preview the Dataset**\n",
    "\n",
    "We start by loading the inherited properties dataset. This dataset contains raw data, and we will need to preprocess it to align with the format of the training and testing datasets used in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inherited Properties Dataset Loaded Successfully.\n",
      "   1stFlrSF  2ndFlrSF  BedroomAbvGr BsmtExposure  BsmtFinSF1 BsmtFinType1  \\\n",
      "0       896         0             2           No       468.0          Rec   \n",
      "1      1329         0             3           No       923.0          ALQ   \n",
      "2       928       701             3           No       791.0          GLQ   \n",
      "3       926       678             3           No       602.0          GLQ   \n",
      "\n",
      "   BsmtUnfSF  EnclosedPorch  GarageArea GarageFinish  ...  LotArea  \\\n",
      "0      270.0              0       730.0          Unf  ...    11622   \n",
      "1      406.0              0       312.0          Unf  ...    14267   \n",
      "2      137.0              0       482.0          Fin  ...    13830   \n",
      "3      324.0              0       470.0          Fin  ...     9978   \n",
      "\n",
      "   LotFrontage MasVnrArea  OpenPorchSF  OverallCond  OverallQual  TotalBsmtSF  \\\n",
      "0         80.0        0.0            0            6            5        882.0   \n",
      "1         81.0      108.0           36            6            6       1329.0   \n",
      "2         74.0        0.0           34            5            5        928.0   \n",
      "3         78.0       20.0           36            6            6        926.0   \n",
      "\n",
      "   WoodDeckSF  YearBuilt  YearRemodAdd  \n",
      "0         140       1961          1961  \n",
      "1         393       1958          1958  \n",
      "2         212       1997          1998  \n",
      "3         360       1998          1998  \n",
      "\n",
      "[4 rows x 23 columns]\n",
      "\n",
      "Inherited Properties Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   1stFlrSF       4 non-null      int64  \n",
      " 1   2ndFlrSF       4 non-null      int64  \n",
      " 2   BedroomAbvGr   4 non-null      int64  \n",
      " 3   BsmtExposure   4 non-null      object \n",
      " 4   BsmtFinSF1     4 non-null      float64\n",
      " 5   BsmtFinType1   4 non-null      object \n",
      " 6   BsmtUnfSF      4 non-null      float64\n",
      " 7   EnclosedPorch  4 non-null      int64  \n",
      " 8   GarageArea     4 non-null      float64\n",
      " 9   GarageFinish   4 non-null      object \n",
      " 10  GarageYrBlt    4 non-null      float64\n",
      " 11  GrLivArea      4 non-null      int64  \n",
      " 12  KitchenQual    4 non-null      object \n",
      " 13  LotArea        4 non-null      int64  \n",
      " 14  LotFrontage    4 non-null      float64\n",
      " 15  MasVnrArea     4 non-null      float64\n",
      " 16  OpenPorchSF    4 non-null      int64  \n",
      " 17  OverallCond    4 non-null      int64  \n",
      " 18  OverallQual    4 non-null      int64  \n",
      " 19  TotalBsmtSF    4 non-null      float64\n",
      " 20  WoodDeckSF     4 non-null      int64  \n",
      " 21  YearBuilt      4 non-null      int64  \n",
      " 22  YearRemodAdd   4 non-null      int64  \n",
      "dtypes: float64(7), int64(12), object(4)\n",
      "memory usage: 864.0+ bytes\n",
      "None\n",
      "\n",
      "Inherited Properties Dataset Preview:\n",
      "   1stFlrSF  2ndFlrSF  BedroomAbvGr BsmtExposure  BsmtFinSF1 BsmtFinType1  \\\n",
      "0       896         0             2           No       468.0          Rec   \n",
      "1      1329         0             3           No       923.0          ALQ   \n",
      "2       928       701             3           No       791.0          GLQ   \n",
      "3       926       678             3           No       602.0          GLQ   \n",
      "\n",
      "   BsmtUnfSF  EnclosedPorch  GarageArea GarageFinish  ...  LotArea  \\\n",
      "0      270.0              0       730.0          Unf  ...    11622   \n",
      "1      406.0              0       312.0          Unf  ...    14267   \n",
      "2      137.0              0       482.0          Fin  ...    13830   \n",
      "3      324.0              0       470.0          Fin  ...     9978   \n",
      "\n",
      "   LotFrontage MasVnrArea  OpenPorchSF  OverallCond  OverallQual  TotalBsmtSF  \\\n",
      "0         80.0        0.0            0            6            5        882.0   \n",
      "1         81.0      108.0           36            6            6       1329.0   \n",
      "2         74.0        0.0           34            5            5        928.0   \n",
      "3         78.0       20.0           36            6            6        926.0   \n",
      "\n",
      "   WoodDeckSF  YearBuilt  YearRemodAdd  \n",
      "0         140       1961          1961  \n",
      "1         393       1958          1958  \n",
      "2         212       1997          1998  \n",
      "3         360       1998          1998  \n",
      "\n",
      "[4 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Inherited properties dataset\n",
    "inherited_properties_path = \"outputs/datasets/raw/inherited_houses.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    inherited_properties = pd.read_csv(inherited_properties_path)\n",
    "    print(\"Inherited Properties Dataset Loaded Successfully.\")\n",
    "    print(inherited_properties.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "# Preview Columns and data types\n",
    "print(\"\\nInherited Properties Dataset Info:\")\n",
    "print(inherited_properties.info())\n",
    "print(\"\\nInherited Properties Dataset Preview:\")\n",
    "print(inherited_properties.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inherited properties dataset contains raw features and requires preprocessing to match the format of the datasets used for training. This includes feature scaling , engineering, and encoding to ensure compatibility with the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Columns:\n",
      "Index(['num__1stFlrSF', 'num__2ndFlrSF', 'num__BedroomAbvGr',\n",
      "       'num__BsmtFinSF1', 'num__BsmtUnfSF', 'num__GarageArea',\n",
      "       'num__GarageYrBlt', 'num__GrLivArea', 'num__LotArea',\n",
      "       'num__LotFrontage', 'num__MasVnrArea', 'num__OpenPorchSF',\n",
      "       'num__OverallCond', 'num__OverallQual', 'num__YearBuilt',\n",
      "       'num__YearRemodAdd', 'num__Age', 'num__LivingLotRatio',\n",
      "       'num__FinishedBsmtRatio', 'num__OverallScore', 'cat__HasPorch'],\n",
      "      dtype='object')\n",
      "\n",
      "Test Features Columns:\n",
      "Index(['num__LotFrontage', 'num__LotArea', 'num__OpenPorchSF',\n",
      "       'num__MasVnrArea', 'num__BsmtFinSF1', 'num__GrLivArea', 'num__1stFlrSF',\n",
      "       'num__YearBuilt', 'num__YearRemodAdd', 'num__BedroomAbvGr',\n",
      "       'num__2ndFlrSF', 'num__BsmtUnfSF', 'num__GarageArea',\n",
      "       'num__GarageYrBlt', 'num__OverallCond', 'num__OverallQual', 'num__Age',\n",
      "       'num__LivingLotRatio', 'num__FinishedBsmtRatio', 'num__OverallScore',\n",
      "       'cat__HasPorch'],\n",
      "      dtype='object')\n",
      "\n",
      "Inherited Properties Dataset Processed Successfully.\n",
      "   num__LotFrontage  num__LotArea  num__OpenPorchSF  num__MasVnrArea  \\\n",
      "0          0.652753     -0.463187         -1.729590        -0.716977   \n",
      "1          1.025755      1.063930          0.620042         1.702821   \n",
      "2         -1.585258      0.811624          0.489506        -0.716977   \n",
      "3         -0.093250     -1.412366          0.620042        -0.268866   \n",
      "\n",
      "   num__BsmtFinSF1  num__GrLivArea  num__1stFlrSF  num__YearBuilt  \\\n",
      "0        -1.308887       -1.588197      -0.691360       -0.919462   \n",
      "1         1.303146       -0.120344       1.727702       -1.077084   \n",
      "2         0.545369        0.896645      -0.512584        0.972003   \n",
      "3        -0.539629        0.811896      -0.523758        1.024544   \n",
      "\n",
      "   num__YearRemodAdd  num__BedroomAbvGr  ...  num__BsmtUnfSF  num__GarageArea  \\\n",
      "0          -0.920681          -1.732051  ...       -0.145650         1.547993   \n",
      "1          -1.076290           0.577350  ...        1.244410        -1.247087   \n",
      "2           0.998485           0.577350  ...       -1.505046        -0.110332   \n",
      "3           0.998485           0.577350  ...        0.406286        -0.190574   \n",
      "\n",
      "   num__GarageYrBlt  num__OverallCond  num__OverallQual  num__Age  \\\n",
      "0         -0.919462          0.577350              -1.0  0.919462   \n",
      "1         -1.077084          0.577350               1.0  1.077084   \n",
      "2          0.972003         -1.732051              -1.0 -0.972003   \n",
      "3          1.024544          0.577350               1.0 -1.024544   \n",
      "\n",
      "   num__LivingLotRatio  num__FinishedBsmtRatio  num__OverallScore  \\\n",
      "0             1.338608               -1.312840          -0.301511   \n",
      "1             0.449660                0.109406           0.904534   \n",
      "2            -0.443035                1.479314          -1.507557   \n",
      "3            -1.345234               -0.275880           0.904534   \n",
      "\n",
      "   cat__HasPorch  \n",
      "0            0.0  \n",
      "1            1.0  \n",
      "2            1.0  \n",
      "3            1.0  \n",
      "\n",
      "[4 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_inherited_properties(raw_data):\n",
    "    \"\"\" \n",
    "        Preprocess the inherited properties dataset to align with the training data format.\n",
    "\n",
    "        Parameters:\n",
    "            raw_data: Raw DataFrame of inherited properties\n",
    "        \n",
    "        Returns:\n",
    "        Processed DataFrame with the same structure as the training features.\n",
    "    \"\"\"\n",
    "    current_year = datetime.datetime.now().year \n",
    "\n",
    "    processed_data = raw_data.copy()\n",
    "\n",
    "    # Drop columns with high missing percentages\n",
    "    processed_data.drop(columns=[\"EnclosedPorch\", \"WoodDeckSF\"], inplace=True)\n",
    "\n",
    "    # One-hot encoding for categorical variables\n",
    "    processed_data = pd.get_dummies(processed_data, columns=[\"GarageFinish\", \"BsmtFinType1\"], drop_first=True)\n",
    "\n",
    "    # Create new features\n",
    "    processed_data[\"num__Age\"] = current_year - processed_data[\"YearBuilt\"]\n",
    "    processed_data[\"num__LivingLotRatio\"] = processed_data[\"LotArea\"] / processed_data[\"GrLivArea\"].replace(0, 1)\n",
    "    processed_data[\"num__FinishedBsmtRatio\"] = processed_data[\"BsmtFinSF1\"] / processed_data[\"TotalBsmtSF\"].replace(0, 1)\n",
    "    processed_data[\"num__OverallScore\"] = processed_data[\"OverallQual\"] + processed_data[\"OverallCond\"]\n",
    "    processed_data[\"cat__HasPorch\"] = (processed_data[\"OpenPorchSF\"].astype(float) > 0).astype(float)\n",
    "\n",
    "    # Drop extra features\n",
    "    extra_features = {'BsmtFinType1_GLQ', 'KitchenQual', 'GarageFinish_Unf', 'BsmtFinType1_Rec', 'BsmtExposure', 'TotalBsmtSF'}\n",
    "    processed_data.drop(columns=extra_features.intersection(processed_data.columns), inplace=True)\n",
    "\n",
    "    # Rename columns to match the test features\n",
    "    processed_data.rename(\n",
    "        columns={\n",
    "            \"1stFlrSF\": \"num__1stFlrSF\",\n",
    "            \"2ndFlrSF\": \"num__2ndFlrSF\",\n",
    "            \"BedroomAbvGr\": \"num__BedroomAbvGr\",\n",
    "            \"BsmtFinSF1\": \"num__BsmtFinSF1\",\n",
    "            \"BsmtUnfSF\": \"num__BsmtUnfSF\",\n",
    "            \"GarageArea\": \"num__GarageArea\",\n",
    "            \"GarageYrBlt\": \"num__GarageYrBlt\",\n",
    "            \"GrLivArea\": \"num__GrLivArea\",\n",
    "            \"LotArea\": \"num__LotArea\",\n",
    "            \"LotFrontage\": \"num__LotFrontage\",\n",
    "            \"MasVnrArea\": \"num__MasVnrArea\",\n",
    "            \"OpenPorchSF\": \"num__OpenPorchSF\",\n",
    "            \"OverallCond\": \"num__OverallCond\",\n",
    "            \"OverallQual\": \"num__OverallQual\",\n",
    "            \"YearBuilt\": \"num__YearBuilt\",\n",
    "            \"YearRemodAdd\": \"num__YearRemodAdd\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Debugging Outputs\n",
    "    print(\"Processed Data Columns:\")\n",
    "    print(processed_data.columns)\n",
    "\n",
    "    print(\"\\nTest Features Columns:\")\n",
    "    print(test_features.columns)\n",
    "\n",
    "    # Align features with training data\n",
    "    missing_features = set(test_features.columns) - set(processed_data.columns)\n",
    "    for col in missing_features:\n",
    "        processed_data[col] = 0\n",
    "    \n",
    "    extra_features = set(processed_data.columns) - set(test_features.columns)\n",
    "    processed_data.drop(columns=extra_features, inplace=True)\n",
    "\n",
    "    # Reorder columns to match training data\n",
    "    processed_data = processed_data[test_features.columns]\n",
    "    assert list(processed_data.columns) == list(test_features.columns), \"Column alignment mismatch!\"\n",
    "\n",
    "    # Apply scaling and encoding\n",
    "    numeric_features = [col for col in processed_data.columns if col.startswith(\"num__\")]\n",
    "\n",
    "    # Standardize numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    processed_data[numeric_features] = scaler.fit_transform(processed_data[numeric_features])\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Apply the preprocessing function\n",
    "try:\n",
    "    inherited_properties_processed = preprocess_inherited_properties(inherited_properties)\n",
    "    print(\"\\nInherited Properties Dataset Processed Successfully.\")\n",
    "    print(inherited_properties_processed.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error processing inherited properties dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Other Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
